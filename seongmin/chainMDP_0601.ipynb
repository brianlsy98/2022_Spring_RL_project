{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dt8wywICwZAC"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uNDKkFNCwEhb"
   },
   "outputs": [],
   "source": [
    "class ChainMDP(gym.Env):\n",
    "    \"\"\"Chain MDP\n",
    "    The environment consists of a chain of N states and the agent always starts in state s2,\n",
    "    from where it can either move left or right.\n",
    "    In state s1, the agent receives a small reward of r = 0.001 by moving left.\n",
    "    A larger reward r = 1 is recived when moving right from state sN.\n",
    "    This environment is described in\n",
    "    Deep Exploration via Bootstrapped DQN(https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.state = 1  # start at s2\n",
    "        self.action_space = spaces.Discrete(2)  # {0, 1}\n",
    "        self.observation_space = spaces.Discrete(self.n)  # {0, 1, ... n-1}\n",
    "        self.max_nsteps = n + 8\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)  # assert 뒤의 조건이 True가 아니면 AssertError 발생\n",
    "        v = np.arange(self.n)  # [0, 1, ... n-1]\n",
    "        reward = lambda s, a: 1.0 if (s == (self.n - 1) and a == 1) else (0.001 if (s == 0 and a == 0) else 0)\n",
    "        is_done = lambda nsteps: nsteps >= self.max_nsteps  # True/False\n",
    "\n",
    "        r = reward(self.state, action)\n",
    "        if action:    # right\n",
    "            if self.state != self.n - 1:\n",
    "                self.state += 1\n",
    "        else:   # left\n",
    "            if self.state != 0:\n",
    "                self.state -= 1\n",
    "        self.nsteps += 1\n",
    "        return (v <= self.state).astype('float32'), r, is_done(self.nsteps), None\n",
    "\n",
    "    def reset(self):\n",
    "        v = np.arange(self.n)\n",
    "        self.state = 1\n",
    "        self.nsteps = 0\n",
    "        return (v <= self.state).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent _chainMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        x = self.input_layer(states)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        ########################################################################\n",
    "\n",
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q = []\n",
    "        for j in range(len(actions)):\n",
    "            q.append(self.qfunction(states)[j][actions[j]])\n",
    "        return tf.convert_to_tensor(q, dtype=tf.float32)\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        l = tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  theta -> theta target \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        # soft assign\n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(0.8*v1+0.2*v2)\n",
    "\n",
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXIbA5RBwTLA"
   },
   "outputs": [],
   "source": [
    "# class agent():\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def action(self):\n",
    "        \n",
    "#         return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation ###\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, e):\n",
    "\n",
    "        self.env = e\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        ### For Q value training ###        \n",
    "        self.episode_length = 100 #10000\n",
    "        self.hidden_dim = [8, 4]\n",
    "        self.lr = 5e-4\n",
    "        self.Qprin = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qtarg = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        ############################\n",
    "\n",
    "        return\n",
    "    \n",
    "    def action(self, s):\n",
    "        self.s = s\n",
    "        Q = self.Qprin.compute_Qvalues(np.array(self.s))\n",
    "        action = np.argmax(Q)   # always max action choose\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        ### For Q value training ###\n",
    "        totalstep = 0\n",
    "        initialize = 500\n",
    "        eps = 1; eps_minus = 1e-4\n",
    "        tau = 100\n",
    "        gamma = 0.99\n",
    "        batchsize = 64\n",
    "        buff_max_size = 10000\n",
    "        buffer = ReplayBuffer(buff_max_size)\n",
    "        ############################\n",
    "\n",
    "        r_record = []\n",
    "\n",
    "        for ite in range(self.episode_length):\n",
    "            self.state = self.env.reset()\n",
    "            done = False\n",
    "            rsum = 0\n",
    "\n",
    "            while not done:\n",
    "                totalstep += 1\n",
    "\n",
    "                if eps > 0.05 and totalstep > initialize: eps -= eps_minus\n",
    "                elif eps < 0.05 and totalstep > initialize: eps = 0.05\n",
    "\n",
    "                ##################\n",
    "                ### Get Action ###\n",
    "                ##################\n",
    "                if np.random.rand() < eps or totalstep <= initialize:\n",
    "                    action = np.random.choice([0, 1])\n",
    "                else:\n",
    "                    Q = self.Qprin.compute_Qvalues(np.array(self.state))\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                ##################\n",
    "\n",
    "                ##################\n",
    "                ###  ONE STEP  ###\n",
    "                ##################\n",
    "                curr_state = self.state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rsum += reward\n",
    "                ##################\n",
    "                \n",
    "                #####################\n",
    "                ### Update Buffer ###\n",
    "                #####################\n",
    "                buffer.append((curr_state, action, reward, next_state, done))\n",
    "                #####################\n",
    "\n",
    "                #############################\n",
    "                ### N Samples from Buffer ###\n",
    "                ###         and           ###\n",
    "                ### Update theta of Qprin ###\n",
    "                #############################\n",
    "                if totalstep > initialize:\n",
    "\n",
    "                    # sample\n",
    "                    s = buffer.sample(batchsize)\n",
    "\n",
    "                    d = []\n",
    "                    for j in range(len(s)):\n",
    "                        cS = s[j][0]; A = s[j][1]; R = s[j][2]; nS = s[j][3]; DONE = s[j][4];\n",
    "                        if not DONE:\n",
    "                            k = R + gamma*np.max(self.Qtarg.compute_Qvalues(nS))\n",
    "                        elif DONE:\n",
    "                            k = R\n",
    "                        d.append(k)\n",
    "                    \n",
    "                    set_of_S = np.array([s[x][0] for x in range(len(s))])\n",
    "                    set_of_A = np.array([s[x][1] for x in range(len(s))])\n",
    "                    loss = self.Qprin.train(set_of_S, set_of_A, tf.convert_to_tensor(d, dtype=tf.float32))\n",
    "                #############################\n",
    "\n",
    "\n",
    "                #############################\n",
    "                ### Update theta of Qtarg ###\n",
    "                #############################\n",
    "                if totalstep % tau == 0:\n",
    "                    print(\"\")\n",
    "                    print(\"epsilon : \", eps)\n",
    "                    print(\"target updated, totalstep : \", totalstep)\n",
    "                    self.Qtarg.update_weights(self.Qprin)\n",
    "                #############################\n",
    "\n",
    "                pass\n",
    "            \n",
    "\n",
    "            r_record.append(rsum)\n",
    "            if ite % 10 == 0:\n",
    "                print('iteration {} ave reward {}'.format(ite, np.mean(r_record[-10:])))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain length\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ave reward 0.002\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  100\n",
      "iteration 10 ave reward 0.0015\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  200\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  300\n",
      "iteration 20 ave reward 0.0009000000000000001\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  400\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  500\n",
      "iteration 30 ave reward 0.0027\n",
      "\n",
      "epsilon :  0.9900000000000011\n",
      "target updated, totalstep :  600\n",
      "\n",
      "epsilon :  0.9800000000000022\n",
      "target updated, totalstep :  700\n",
      "iteration 40 ave reward 0.10169999999999997\n",
      "\n",
      "epsilon :  0.9700000000000033\n",
      "target updated, totalstep :  800\n",
      "\n",
      "epsilon :  0.9600000000000044\n",
      "target updated, totalstep :  900\n",
      "iteration 50 ave reward 0.5017\n",
      "\n",
      "epsilon :  0.9500000000000055\n",
      "target updated, totalstep :  1000\n",
      "iteration 60 ave reward 0.0023\n",
      "\n",
      "epsilon :  0.9400000000000066\n",
      "target updated, totalstep :  1100\n",
      "\n",
      "epsilon :  0.9300000000000077\n",
      "target updated, totalstep :  1200\n",
      "iteration 70 ave reward 0.40140000000000003\n",
      "\n",
      "epsilon :  0.9200000000000088\n",
      "target updated, totalstep :  1300\n",
      "\n",
      "epsilon :  0.9100000000000099\n",
      "target updated, totalstep :  1400\n",
      "iteration 80 ave reward 0.10089999999999999\n",
      "\n",
      "epsilon :  0.900000000000011\n",
      "target updated, totalstep :  1500\n",
      "\n",
      "epsilon :  0.8900000000000121\n",
      "target updated, totalstep :  1600\n",
      "iteration 90 ave reward 0.40170000000000006\n",
      "\n",
      "epsilon :  0.8800000000000132\n",
      "target updated, totalstep :  1700\n",
      "\n",
      "epsilon :  0.8700000000000143\n",
      "target updated, totalstep :  1800\n"
     ]
    }
   ],
   "source": [
    "# from chain_mdp import ChainMDP\n",
    "# from agent_chainMDP import agent\n",
    "\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent(env)     # agent call\n",
    "agent.train()          # train policy of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "##### eval code #####\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "# always move right left: 0, right: 1\n",
    "\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.action(s)\n",
    "    print(\"state: \", s)\n",
    "    print(\"action: \", action)\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    s = ns\n",
    "    \n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpfCYx8n6uiaJryHpETH/m",
   "name": "chainMDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
