{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dt8wywICwZAC"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uNDKkFNCwEhb"
   },
   "outputs": [],
   "source": [
    "class ChainMDP(gym.Env):\n",
    "    \"\"\"Chain MDP\n",
    "    The environment consists of a chain of N states and the agent always starts in state s2,\n",
    "    from where it can either move left or right.\n",
    "    In state s1, the agent receives a small reward of r = 0.001 by moving left.\n",
    "    A larger reward r = 1 is recived when moving right from state sN.\n",
    "    This environment is described in\n",
    "    Deep Exploration via Bootstrapped DQN(https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.state = 1  # start at s2\n",
    "        self.action_space = spaces.Discrete(2)  # {0, 1}\n",
    "        self.observation_space = spaces.Discrete(self.n)  # {0, 1, ... n-1}\n",
    "        self.max_nsteps = n + 8\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)  # assert 뒤의 조건이 True가 아니면 AssertError 발생\n",
    "        v = np.arange(self.n)  # [0, 1, ... n-1]\n",
    "        reward = lambda s, a: 1.0 if (s == (self.n - 1) and a == 1) else (0.001 if (s == 0 and a == 0) else 0)\n",
    "        is_done = lambda nsteps: nsteps >= self.max_nsteps  # True/False\n",
    "\n",
    "        r = reward(self.state, action)\n",
    "        if action:    # right\n",
    "            if self.state != self.n - 1:\n",
    "                self.state += 1\n",
    "        else:   # left\n",
    "            if self.state != 0:\n",
    "                self.state -= 1\n",
    "        self.nsteps += 1\n",
    "        return (v <= self.state).astype('float32'), r, is_done(self.nsteps), None\n",
    "\n",
    "    def reset(self):\n",
    "        v = np.arange(self.n)\n",
    "        self.state = 1\n",
    "        self.nsteps = 0\n",
    "        return (v <= self.state).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent _chainMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        x = self.input_layer(states)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        ########################################################################\n",
    "\n",
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q = []\n",
    "        for j in range(len(actions)):\n",
    "            q.append(self.qfunction(states)[j][actions[j]])\n",
    "        return tf.convert_to_tensor(q, dtype=tf.float32)\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        l = tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  theta -> theta target \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        # soft assign\n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(0.8*v1+0.2*v2)\n",
    "\n",
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXIbA5RBwTLA"
   },
   "outputs": [],
   "source": [
    "# class agent():\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def action(self):\n",
    "        \n",
    "#         return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation ###\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, e, head_num):\n",
    "\n",
    "        self.env = e\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        ### For Q value training ###        \n",
    "        self.episode_length = 100 #10000\n",
    "        self.hidden_dim = [8, 4]\n",
    "        self.lr = 5e-4\n",
    "        \n",
    "        self.bernoulli_prob = 0.9\n",
    "        self.ensemble_num = head_num\n",
    "        \n",
    "        self.Qprin = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qtarg = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qs = []\n",
    "        for _ in range(self.ensemble_num):\n",
    "            self.Qs.append([self.Qprin, self.Qtarg])\n",
    "        ############################\n",
    "\n",
    "        return\n",
    "    \n",
    "    def action(self, s):\n",
    "        self.s = s\n",
    "        voting_paper = np.zeros(self.env.action_space.n)\n",
    "        \n",
    "        for n in range(self.ensemble_num):\n",
    "            Q = self.Qs[n][0].compute_Qvalues(np.array(self.s))\n",
    "            action = np.argmax(Q)   # always max action choose\n",
    "            # voting_paper[action] += 1\n",
    "            voting_paper[action] += Q[0][action] - np.mean(Q[0])\n",
    "        \n",
    "\n",
    "        return np.argmax(voting_paper)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        ### For Q value training ###\n",
    "        totalstep = 0\n",
    "        initialize = 500\n",
    "        eps = 1; eps_minus = 1e-4\n",
    "        tau = 100\n",
    "        gamma = 0.99\n",
    "        batchsize = 64\n",
    "        buff_max_size = 10000\n",
    "        buffer = ReplayBuffer(buff_max_size)\n",
    "        ############################\n",
    "\n",
    "        self.r_record = []\n",
    "        self.AUC = []\n",
    "\n",
    "        for ite in range(self.episode_length):\n",
    "            self.state = self.env.reset()\n",
    "            done = False\n",
    "            rsum = 0\n",
    "\n",
    "            # Action :\n",
    "            # - train : head fixed for each epoch\n",
    "            # - eval : vote\n",
    "            head4action_train = np.random.randint(0, self.ensemble_num)\n",
    "            \n",
    "            while not done:\n",
    "                totalstep += 1\n",
    "\n",
    "                if eps > 0.05 and totalstep > initialize: eps -= eps_minus\n",
    "                elif eps < 0.05 and totalstep > initialize: eps = 0.05\n",
    "\n",
    "                ##################\n",
    "                ### Get Action ###\n",
    "                ##################\n",
    "                if np.random.rand() < eps or totalstep <= initialize:\n",
    "                    action = np.random.choice([0, 1])\n",
    "                else:\n",
    "                    Q = self.Qs[head4action_train][0].compute_Qvalues(np.array(self.state)) # Qprin\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                ##################\n",
    "\n",
    "                ##################\n",
    "                ###  ONE STEP  ###\n",
    "                ##################\n",
    "                curr_state = self.state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rsum += reward\n",
    "                ##################\n",
    "                \n",
    "                # === ensemble DQN === #\n",
    "                heads = np.random.binomial(1, self.bernoulli_prob, self.ensemble_num)\n",
    "                if np.sum(heads) == 0:\n",
    "                    heads[np.random.randint(0, self.ensemble_num)] = 1\n",
    "                # ==================== #\n",
    "                \n",
    "                #####################\n",
    "                ### Update Buffer ###\n",
    "                #####################\n",
    "                buffer.append((curr_state, action, reward, next_state, done, heads))\n",
    "                #####################\n",
    "\n",
    "                #############################\n",
    "                ### N Samples from Buffer ###\n",
    "                ###         and           ###\n",
    "                ### Update theta of Qprin ###\n",
    "                #############################\n",
    "                if totalstep > initialize:\n",
    "\n",
    "                    # sample\n",
    "                    s = buffer.sample(batchsize)\n",
    "\n",
    "                    d = []\n",
    "                    for j in range(len(s)): # for each s[j]s\n",
    "                        ## if head 0 : append 0 at K, head 1 : append value of k to K\n",
    "                        ## iterate for all samples\n",
    "                        K=[]\n",
    "                        cS = s[j][0]; A = s[j][1]; R = s[j][2]; nS = s[j][3]; DONE = s[j][4]; heads = s[j][5];\n",
    "                        if not DONE:\n",
    "                            for n in range(len(heads)):\n",
    "                                if heads[n] == 0 : k = 0\n",
    "                                else: k = R + gamma*np.max(self.Qs[n][1].compute_Qvalues(nS)) #Qtarg_n\n",
    "                                K.append(k)\n",
    "                        elif DONE:\n",
    "                            for n in range(len(heads)):\n",
    "                                if heads[n] == 0 : k = 0\n",
    "                                else: k = R\n",
    "                                K.append(k)\n",
    "                        d.append(K)\n",
    "                    \n",
    "                    # update Qprins\n",
    "                    for n in range(self.ensemble_num):\n",
    "                        set_of_S = np.array([s[x][0] for x in range(len(s)) if s[x][5][n] == 1])\n",
    "                        set_of_A = np.array([s[x][1] for x in range(len(s)) if s[x][5][n] == 1])\n",
    "                        D = [d[x][n] for x in range(len(s)) if s[x][5][n] == 1]\n",
    "                        \n",
    "                        self.Qs[n][0].train(set_of_S, set_of_A, tf.convert_to_tensor(D, dtype=tf.float32))  # Qprin\n",
    "                #############################\n",
    "\n",
    "\n",
    "                #############################\n",
    "                ### Update theta of Qtarg ###\n",
    "                #############################\n",
    "                if totalstep % tau == 0:\n",
    "#                     print(\"\")\n",
    "#                     print(\"epsilon : \", eps)\n",
    "#                     print(\"target updated, totalstep : \", totalstep)\n",
    "                    for n in range(self.ensemble_num):\n",
    "                        self.Qs[n][1].update_weights(self.Qs[n][0])\n",
    "\n",
    "                #############################\n",
    "\n",
    "                pass\n",
    "            \n",
    "\n",
    "            self.r_record.append(rsum)\n",
    "#             if ite % 10 == 0:\n",
    "#                 print('iteration {} ave reward {}'.format(ite, np.mean(self.r_record[-10:])))\n",
    "            \n",
    "            #########################\n",
    "            ### Sample Efficiency ###\n",
    "            #########################\n",
    "            done = False\n",
    "            cum_reward = 0.0\n",
    "            s = self.env.reset()\n",
    "            while not done:\n",
    "                \n",
    "                voting_paper = np.zeros(self.env.action_space.n)\n",
    "\n",
    "                for n in range(self.ensemble_num):\n",
    "                    Q = self.Qs[n][0].compute_Qvalues(np.array(s))\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                    # voting_paper[action] += 1\n",
    "                    voting_paper[action] += Q[0][action] - np.mean(Q[0])\n",
    "                    \n",
    "                action = np.argmax(voting_paper)\n",
    "                ns, reward, done, _ = env.step(action)\n",
    "                cum_reward += reward\n",
    "                s = ns\n",
    "            self.AUC.append(cum_reward)\n",
    "\n",
    "        return self.AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain length: 10\n",
      "seed: 0\n",
      "AUC:  0.0\n",
      "seed: 1\n",
      "AUC:  0.5100000000000002\n",
      "seed: 2\n",
      "AUC:  0.5950000000000003\n",
      "seed: 3\n",
      "AUC:  1000.0\n",
      "seed: 4\n",
      "AUC:  1000.0\n",
      "seed: 5\n",
      "AUC:  560.0\n",
      "seed: 6\n",
      "AUC:  890.0\n",
      "seed: 7\n",
      "AUC:  1.2070000000000003\n",
      "seed: 8\n",
      "AUC:  910.0\n",
      "seed: 9\n",
      "AUC:  0.4930000000000002\n",
      "seed: 10\n",
      "AUC:  190.11899999999997\n",
      "seed: 11\n",
      "AUC:  0.5100000000000002\n",
      "seed: 12\n",
      "AUC:  0.017000000000000008\n",
      "seed: 13\n",
      "AUC:  590.0\n",
      "seed: 14\n",
      "AUC:  411.00300000000016\n",
      "seed: 15\n",
      "AUC:  950.0\n",
      "seed: 16\n",
      "AUC:  150.204\n",
      "seed: 17\n",
      "AUC:  280.0\n",
      "seed: 18\n",
      "AUC:  480.0\n",
      "seed: 19\n",
      "AUC:  300.0\n",
      "chain length: 50\n",
      "seed: 0\n",
      "AUC:  0.0\n",
      "seed: 1\n",
      "AUC:  0.5130000000000003\n",
      "seed: 2\n",
      "AUC:  0.6840000000000005\n",
      "seed: 3\n",
      "AUC:  1000.0\n",
      "seed: 4\n",
      "AUC:  980.0\n",
      "seed: 5\n",
      "AUC:  0.0\n",
      "seed: 6\n",
      "AUC:  1000.0\n",
      "seed: 7\n",
      "AUC:  4.845000000000004\n",
      "seed: 8\n",
      "AUC:  850.0\n",
      "seed: 9\n",
      "AUC:  0.5130000000000003\n",
      "seed: 10\n",
      "AUC:  770.171\n",
      "seed: 11\n",
      "AUC:  0.7980000000000006\n",
      "seed: 12\n",
      "AUC:  0.0\n",
      "seed: 13\n",
      "AUC:  640.0\n",
      "seed: 14\n",
      "AUC:  150.0\n",
      "seed: 15\n",
      "AUC:  890.0\n",
      "seed: 16\n",
      "AUC:  790.2280000000001\n",
      "seed: 17\n",
      "AUC:  0.0\n",
      "seed: 18\n",
      "AUC:  880.0\n",
      "seed: 19\n",
      "AUC:  110.0\n",
      "chain length: 100\n",
      "seed: 0\n",
      "AUC:  0.0\n",
      "seed: 1\n",
      "AUC:  510.53499999999997\n",
      "seed: 2\n",
      "AUC:  0.6420000000000005\n",
      "seed: 3\n",
      "AUC:  1000.0\n",
      "seed: 4\n",
      "AUC:  990.0\n",
      "seed: 5\n",
      "AUC:  0.0\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    <ipython-input-3-65a570de21a5>:35 call  *\n        x = hidden_layer(x)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:1207 call\n        return core_ops.dense(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py:53 dense\n        outputs = gen_math_ops.mat_mul(inputs, kernel)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:5547 mat_mul\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:517 _apply_op_helper\n        values = ops.convert_to_tensor(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1992 _dense_var_to_tensor\n        return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1393 _dense_var_to_tensor\n        return self.value()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:565 value\n        return self._read_variable_op()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:672 _read_variable_op\n        result = read_and_set_handle()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:662 read_and_set_handle\n        result = gen_resource_variable_ops.read_variable_op(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:483 read_variable_op\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:517 _apply_op_helper\n        values = ops.convert_to_tensor(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1501 convert_to_tensor\n        return graph.capture(value, name=name)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:626 capture\n        return self._capture_helper(tensor, name, shape)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:658 _capture_helper\n        tape.record_operation(\"captured_value\", [placeholder], [tensor],\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\tape.py:184 record_operation\n        pywrap_tfe.TFE_Py_TapeSetRecordOperation(op_type, output_tensors,\n\n    OverflowError: Python int too large to convert to C long\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-20140b56b093>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mAUC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#iteration: 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"seed:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AUC: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAUC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-29a85de35fc7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_of_S\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_of_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Qprin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;31m#############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-65a570de21a5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, states, actions, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mQpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-65a570de21a5>\u001b[0m in \u001b[0;36m_predict_q\u001b[1;34m(self, states, actions)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3885\u001b[0m     \u001b[1;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m     \u001b[1;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStagingError\u001b[0m: in user code:\n\n    <ipython-input-3-65a570de21a5>:35 call  *\n        x = hidden_layer(x)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__  **\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:1207 call\n        return core_ops.dense(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\ops\\core.py:53 dense\n        outputs = gen_math_ops.mat_mul(inputs, kernel)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:5547 mat_mul\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:517 _apply_op_helper\n        values = ops.convert_to_tensor(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1992 _dense_var_to_tensor\n        return var._dense_var_to_tensor(dtype=dtype, name=name, as_ref=as_ref)  # pylint: disable=protected-access\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1393 _dense_var_to_tensor\n        return self.value()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:565 value\n        return self._read_variable_op()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:672 _read_variable_op\n        result = read_and_set_handle()\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:662 read_and_set_handle\n        result = gen_resource_variable_ops.read_variable_op(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:483 read_variable_op\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:517 _apply_op_helper\n        values = ops.convert_to_tensor(\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1501 convert_to_tensor\n        return graph.capture(value, name=name)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:626 capture\n        return self._capture_helper(tensor, name, shape)\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:658 _capture_helper\n        tape.record_operation(\"captured_value\", [placeholder], [tensor],\n    C:\\Users\\seongmin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\tape.py:184 record_operation\n        pywrap_tfe.TFE_Py_TapeSetRecordOperation(op_type, output_tensors,\n\n    OverflowError: Python int too large to convert to C long\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed): # seed 고정\n",
    "    np.random.seed(seed)\n",
    "#    random.seed(seed)\n",
    "#    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "head_num = 5\n",
    "seeds = 20\n",
    "for n in [10, 50, 100]:\n",
    "    print(\"chain length:\", n)\n",
    "    agents = []\n",
    "    \n",
    "    for seed in range(seeds):\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        env = ChainMDP(n)\n",
    "        s = env.reset()\n",
    "        agents.append(agent(env, head_num))\n",
    "        AUC = agents[seed].train() #iteration: 100\n",
    "        print(\"seed:\", seed)\n",
    "        print(\"AUC: \", np.sum(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain length\n",
    "n = 10\n",
    "head_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from chain_mdp import ChainMDP\n",
    "# from agent_chainMDP import agent\n",
    "\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent(env, head_num)     # agent call\n",
    "agent.train()          # train policy of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### eval code #####\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "# always move right left: 0, right: 1\n",
    "\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.action(s)\n",
    "    print(\"state: \", s)\n",
    "    print(\"action: \", action)\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    s = ns\n",
    "    \n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpfCYx8n6uiaJryHpETH/m",
   "name": "chainMDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
