
TEAM : 5
- HyeokJin Kwon
- Seongmin Lee
- Sungyoung Lee

Training Explanation:
- mode selection is included when making new agent()
    
    1) Training mode (1)
     * if you type 1, the agent starts to train from the beginning
        - After typing 1, you can initialize weights of the Keras Models randomly
        depending on the seed you enter (integer from 1 to 10)
        - Training goes on for 50 epochs, and at each epoch there are 4000 steps for default
        (even if done=True is returned, each epoch is not finished until 4000 steps)
     * Training Result : converges at about 30 epoch or a little more
    
    2) Evaluation mode (2)
     * if you type 2, the agent starts to load saved model, and it is evaluated based on the loaded model
        - Saved Model(folder) is attatched in the zip file
        - Since we used "os.getcwd()" command, the "model" folder and "lava_test.py", "agent_lava.py"
        should exist in the same folder, and TAs should run test code in the directory of the lava_test.py file



Very litte implementation in lava_test.py file:

***************************************
***************************************
import gym
from lava_grid import ZigZag6x10
from agent_lava import agent
import random

# default setting
max_steps = 100
stochasticity = 0 # probability of the selected action failing and instead executing any of the remaining 3
no_render = True

env = ZigZag6x10(max_steps=max_steps, act_fail_prob=stochasticity, goal=(5, 9), numpy_state=False)
s = env.reset()
done = False
cum_reward = 0.0

""" Your agent"""
agent = agent()

#######################################
# === For training from beginning === #
if agent.mode == 1 : agent.train(env) #     <-- what's added
# =================================== #
#######################################

# moving costs -0.01, falling in lava -1, reaching goal +1
# final reward is number_of_steps / max_steps
while not done:
    action = agent.action(s)
    # action = random.randrange(4): random actions
    ns, reward, done, _ = env.step(action)
    cum_reward += reward
    s = ns
print(f"total reward: {cum_reward}")
***************************************
***************************************