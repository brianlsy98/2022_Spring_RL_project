{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dt8wywICwZAC"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uNDKkFNCwEhb"
   },
   "outputs": [],
   "source": [
    "class ChainMDP(gym.Env):\n",
    "    \"\"\"Chain MDP\n",
    "    The environment consists of a chain of N states and the agent always starts in state s2,\n",
    "    from where it can either move left or right.\n",
    "    In state s1, the agent receives a small reward of r = 0.001 by moving left.\n",
    "    A larger reward r = 1 is recived when moving right from state sN.\n",
    "    This environment is described in\n",
    "    Deep Exploration via Bootstrapped DQN(https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.state = 1  # start at s2\n",
    "        self.action_space = spaces.Discrete(2)  # {0, 1}\n",
    "        self.observation_space = spaces.Discrete(self.n)  # {0, 1, ... n-1}\n",
    "        self.max_nsteps = n + 8\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)  # assert 뒤의 조건이 True가 아니면 AssertError 발생\n",
    "        v = np.arange(self.n)  # [0, 1, ... n-1]\n",
    "        reward = lambda s, a: 1.0 if (s == (self.n - 1) and a == 1) else (0.001 if (s == 0 and a == 0) else 0)\n",
    "        is_done = lambda nsteps: nsteps >= self.max_nsteps  # True/False\n",
    "\n",
    "        r = reward(self.state, action)\n",
    "        if action:    # right\n",
    "            if self.state != self.n - 1:\n",
    "                self.state += 1\n",
    "        else:   # left\n",
    "            if self.state != 0:\n",
    "                self.state -= 1\n",
    "        self.nsteps += 1\n",
    "        return (v <= self.state).astype('float32'), r, is_done(self.nsteps), None\n",
    "\n",
    "    def reset(self):\n",
    "        v = np.arange(self.n)\n",
    "        self.state = 1\n",
    "        self.nsteps = 0\n",
    "        return (v <= self.state).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent _chainMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize) \n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        x = self.input_layer(states)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        ########################################################################\n",
    "\n",
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q = []\n",
    "        for j in range(len(actions)):\n",
    "           q.append(self.qfunction(states)[j][actions[j]])\n",
    "        return tf.convert_to_tensor(q, dtype=tf.float32)\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        l = tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  theta -> theta target \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        # soft assign\n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(0.8*v1+0.2*v2)\n",
    "\n",
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXIbA5RBwTLA"
   },
   "outputs": [],
   "source": [
    "# class agent():\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def action(self):\n",
    "        \n",
    "#         return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation ###\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, e):\n",
    "\n",
    "        self.env = e\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        ### For Q value training ###        \n",
    "        self.episode_length = 1000\n",
    "        self.hidden_dim = [8, 4]\n",
    "        self.lr = 5e-4\n",
    "        self.Qprin = DQN(10, e.action_space.n, [8, 4], optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qtarg = DQN(10, e.action_space.n, [8, 4], optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        ############################\n",
    "\n",
    "        return\n",
    "    \n",
    "    def action(self):\n",
    "        \n",
    "        Q = self.Qprin.compute_Qvalues(np.array(self.state))\n",
    "        action = np.argmax(Q)   # always max action choose\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        ### For Q value training ###\n",
    "        totalstep = 0\n",
    "        initialize = 500\n",
    "        eps = 1; eps_minus = 1e-4\n",
    "        tau = 100\n",
    "        gamma = 0.99\n",
    "        batchsize = 64\n",
    "        buff_max_size = 10000\n",
    "        buffer = ReplayBuffer(buff_max_size)\n",
    "        ############################\n",
    "\n",
    "        r_record = []\n",
    "\n",
    "        for iter in range(self.episode_length):\n",
    "            self.state = self.env.reset()\n",
    "            done = False\n",
    "            rsum = 0\n",
    "\n",
    "            while not done:\n",
    "                totalstep += 1\n",
    "\n",
    "                if eps > 0.05 and totalstep > initialize: eps -= eps_minus\n",
    "                elif eps < 0.05 and totalstep > initialize: eps = 0.05\n",
    "\n",
    "                ##################\n",
    "                ### Get Action ###\n",
    "                ##################\n",
    "                if np.random.rand() < eps or totalstep <= initialize:\n",
    "                    action = np.random.choice([0, 1])\n",
    "                else:\n",
    "                    Q = self.Qprin.compute_Qvalues(np.array(self.state))\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                ##################\n",
    "\n",
    "                ##################\n",
    "                ###  ONE STEP  ###\n",
    "                ##################\n",
    "                curr_state = self.state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rsum += reward\n",
    "                ##################\n",
    "                \n",
    "                #####################\n",
    "                ### Update Buffer ###\n",
    "                #####################\n",
    "                buffer.append((curr_state, action, reward, next_state, done))\n",
    "                #####################\n",
    "\n",
    "                #############################\n",
    "                ### N Samples from Buffer ###\n",
    "                ###         and           ###\n",
    "                ### Update theta of Qprin ###\n",
    "                #############################\n",
    "                if totalstep > initialize:\n",
    "                    # sample\n",
    "                    s = buffer.sample(batchsize)\n",
    "\n",
    "                    d = []\n",
    "                    for j in range(len(s)):\n",
    "                        cS = s[j][0]; A = s[j][1]; R = s[j][2]; nS = s[j][3]; DONE = s[j][4]\n",
    "                        if not DONE:\n",
    "                            k = R + gamma*np.max(self.Qtarg.compute_Qvalues(np.array(nS)))\n",
    "                        elif DONE:\n",
    "                            k = R\n",
    "                        d.append(k)\n",
    "                    \n",
    "                    set_of_S = np.array([s[x][0] for x in range(len(s))])\n",
    "                    set_of_A = np.array([s[x][1] for x in range(len(s))])\n",
    "                    loss = self.Qprin.train(set_of_S, set_of_A, tf.convert_to_tensor(d, dtype=tf.float32))\n",
    "                #############################\n",
    "\n",
    "\n",
    "                #############################\n",
    "                ### Update theta of Qtarg ###\n",
    "                #############################\n",
    "                if totalstep % tau == 0:\n",
    "                    print(\"\")\n",
    "                    print(\"epsilon : \", eps)\n",
    "                    print(\"target updated, totalstep : \", totalstep)\n",
    "                    self.Qtarg.update_weights(self.Qprin)\n",
    "                #############################\n",
    "\n",
    "                pass\n",
    "            \n",
    "\n",
    "            r_record.append(rsum)\n",
    "            if iter % 10 == 0:\n",
    "                print('iteration {} ave reward {}'.format(iter, np.mean(r_record[-10:])))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653541582188,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "Q2AxqGtmwVO3",
    "outputId": "b4a725c2-fcfa-4b78-a591-2bdf8a152aa9"
   },
   "outputs": [],
   "source": [
    "# from chain_mdp import ChainMDP\n",
    "# from agent_chainMDP import agent\n",
    "\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "env = ChainMDP(10)\n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ave reward 0.007\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  100\n",
      "iteration 10 ave reward 0.0024000000000000002\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  200\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  300\n",
      "iteration 20 ave reward 0.0008\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  400\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  500\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(1, 10), b.shape=(10, 8), m=1, n=8, k=10\n\t [[node dense_3/MatMul (defined at <ipython-input-3-1919b047b2b7>:35) ]] [Op:__inference_call_111]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node dense_3/MatMul:\n states (defined at <ipython-input-3-1919b047b2b7>:86)\n\nFunction call stack:\ncall\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8f527ce8a222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\" Your agent\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# agent call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# train policy of the agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-c3a7a3ae7161>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m                         \u001b[0mcS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mnS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mDONE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mDONE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQtarg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_Qvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                         \u001b[1;32melif\u001b[0m \u001b[0mDONE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1919b047b2b7>\u001b[0m in \u001b[0;36mcompute_Qvalues\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \"\"\"\n\u001b[0;32m     85\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(1, 10), b.shape=(10, 8), m=1, n=8, k=10\n\t [[node dense_3/MatMul (defined at <ipython-input-3-1919b047b2b7>:35) ]] [Op:__inference_call_111]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node dense_3/MatMul:\n states (defined at <ipython-input-3-1919b047b2b7>:86)\n\nFunction call stack:\ncall\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent(env)     # agent call\n",
    "agent.train()          # train policy of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### eval code #####\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "# always move right left: 0, right: 1\n",
    "action = 1\n",
    "\n",
    "while not done:    \n",
    "    action = agent.action()\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    s = ns\n",
    "    \n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpfCYx8n6uiaJryHpETH/m",
   "name": "chainMDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
