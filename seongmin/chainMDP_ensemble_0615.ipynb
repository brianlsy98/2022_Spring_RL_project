{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3149,
     "status": "ok",
     "timestamp": 1655253718626,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "dt8wywICwZAC"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RODen-jIwSlL"
   },
   "source": [
    "chain_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1655253718626,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "uNDKkFNCwEhb"
   },
   "outputs": [],
   "source": [
    "class ChainMDP(gym.Env):\n",
    "    \"\"\"Chain MDP\n",
    "    The environment consists of a chain of N states and the agent always starts in state s2,\n",
    "    from where it can either move left or right.\n",
    "    In state s1, the agent receives a small reward of r = 0.001 by moving left.\n",
    "    A larger reward r = 1 is recived when moving right from state sN.\n",
    "    This environment is described in\n",
    "    Deep Exploration via Bootstrapped DQN(https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.state = 1  # start at s2\n",
    "        self.action_space = spaces.Discrete(2)  # {0, 1}\n",
    "        self.observation_space = spaces.Discrete(self.n)  # {0, 1, ... n-1}\n",
    "        self.max_nsteps = n + 8\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)  # assert 뒤의 조건이 True가 아니면 AssertError 발생\n",
    "        v = np.arange(self.n)  # [0, 1, ... n-1]\n",
    "        reward = lambda s, a: 1.0 if (s == (self.n - 1) and a == 1) else (0.001 if (s == 0 and a == 0) else 0)\n",
    "        is_done = lambda nsteps: nsteps >= self.max_nsteps  # True/False\n",
    "\n",
    "        r = reward(self.state, action)\n",
    "        if action:    # right\n",
    "            if self.state != self.n - 1:\n",
    "                self.state += 1\n",
    "        else:   # left\n",
    "            if self.state != 0:\n",
    "                self.state -= 1\n",
    "        self.nsteps += 1\n",
    "        return (v <= self.state).astype('float32'), r, is_done(self.nsteps), None\n",
    "\n",
    "    def reset(self):\n",
    "        v = np.arange(self.n)\n",
    "        self.state = 1\n",
    "        self.nsteps = 0\n",
    "        return (v <= self.state).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nwpvhgOwSlN"
   },
   "source": [
    "agent _chainMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1655253719068,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "y_unhNHdwSlN"
   },
   "outputs": [],
   "source": [
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1e-2, maxval=1e-2)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        x = self.input_layer(states)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        ########################################################################\n",
    "\n",
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q = []\n",
    "        for j in range(len(actions)):\n",
    "            q.append(self.qfunction(states)[j][actions[j]])\n",
    "        return tf.convert_to_tensor(q, dtype=tf.float32)\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        l = tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  theta -> theta target \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        # soft assign\n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(0.8*v1+0.2*v2)\n",
    "\n",
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1655253719068,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "pXIbA5RBwTLA"
   },
   "outputs": [],
   "source": [
    "# class agent():\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def action(self):\n",
    "        \n",
    "#         return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1655253719488,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "hUwQgqwCwSlP"
   },
   "outputs": [],
   "source": [
    "### DQN implementation ###\n",
    "\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, e):\n",
    "\n",
    "        self.env = e\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        ### For Q value training ###        \n",
    "        self.episode_length = 1000 #10000\n",
    "        self.hidden_dim = [8, 4]\n",
    "        self.lr = 5e-4\n",
    "        self.eps =1\n",
    "        \n",
    "        self.bernoulli_prob = 0.6\n",
    "        self.ensemble_num = 3\n",
    "        \n",
    "        self.Qprin = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qtarg = DQN(self.env.n, self.env.action_space.n, self.hidden_dim, optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qs = []\n",
    "        for _ in range(self.ensemble_num):\n",
    "            self.Qs.append([self.Qprin, self.Qtarg])\n",
    "        ############################\n",
    "\n",
    "        return\n",
    "    \n",
    "    def action(self, s):\n",
    "        self.s = s\n",
    "        voting_paper = np.zeros(self.env.action_space.n)\n",
    "        \n",
    "        if np.random.rand() < self.eps:\n",
    "            return np.random.choice([0, 1])\n",
    "        else:\n",
    "            for n in range(self.ensemble_num):\n",
    "                Q = self.Qs[n][0].compute_Qvalues(np.array(self.s))\n",
    "                action = np.argmax(Q)   # always max action choose\n",
    "                # voting_paper[action] += 1\n",
    "                voting_paper[action] += (Q[0][action] - np.mean(Q[0]))/np.std(Q[0]) \n",
    "            return np.argmax(voting_paper)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        ### For Q value training ###\n",
    "        totalstep = 0\n",
    "        initialize = 500\n",
    "        eps = 1; eps_minus = 1e-4\n",
    "        tau = 100\n",
    "        gamma = 0.99\n",
    "        batchsize = 64\n",
    "        buff_max_size = 10000\n",
    "        buffer = ReplayBuffer(buff_max_size)\n",
    "        ############################\n",
    "\n",
    "        self.r_record = []\n",
    "        self.AUC = []\n",
    "\n",
    "        for ite in tqdm(range(self.episode_length)):\n",
    "            self.state = self.env.reset()\n",
    "            done = False\n",
    "            rsum = 0\n",
    "\n",
    "            # Action :\n",
    "            # - train : head fixed for each epoch\n",
    "            # - eval : vote\n",
    "            head4action_train = np.random.randint(0, self.ensemble_num)\n",
    "            \n",
    "            while not done:\n",
    "                totalstep += 1\n",
    "\n",
    "                if eps > 0.05 and totalstep > initialize: eps -= eps_minus\n",
    "                elif eps < 0.05 and totalstep > initialize: eps = 0.05\n",
    "\n",
    "                ##################\n",
    "                ### Get Action ###\n",
    "                ##################\n",
    "                if np.random.rand() < eps or totalstep <= initialize:\n",
    "                    action = np.random.choice([0, 1])\n",
    "                else:\n",
    "#                     Q = self.Qs[head4action_train][0].compute_Qvalues(np.array(self.state)) # Qprin\n",
    "#                     action = np.argmax(Q)   # always max action choose\n",
    "                    voting_paper = np.zeros(self.env.action_space.n)\n",
    "\n",
    "                    for n in range(self.ensemble_num):\n",
    "                        Q = self.Qs[n][0].compute_Qvalues(np.array(self.state))\n",
    "                        action = np.argmax(Q)   # always max action choose\n",
    "                        # voting_paper[action] += 1\n",
    "                        voting_paper[action] += (Q[0][action] - np.mean(Q[0]))/np.std(Q[0])\n",
    "\n",
    "                    action = np.argmax(voting_paper)\n",
    "                ##################\n",
    "\n",
    "                ##################\n",
    "                ###  ONE STEP  ###\n",
    "                ##################\n",
    "                curr_state = self.state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rsum += reward\n",
    "                ##################\n",
    "                \n",
    "                # === ensemble DQN === #\n",
    "                heads = np.random.binomial(1, self.bernoulli_prob, self.ensemble_num)\n",
    "                if np.sum(heads) == 0:\n",
    "                    heads[np.random.randint(0, self.ensemble_num)] = 1\n",
    "                # ==================== #\n",
    "                \n",
    "                #####################\n",
    "                ### Update Buffer ###\n",
    "                #####################\n",
    "                buffer.append((curr_state, action, reward, next_state, done, heads))\n",
    "                #####################\n",
    "\n",
    "                #############################\n",
    "                ### N Samples from Buffer ###\n",
    "                ###         and           ###\n",
    "                ### Update theta of Qprin ###\n",
    "                #############################\n",
    "                if totalstep > initialize:\n",
    "\n",
    "                    # sample\n",
    "                    s = buffer.sample(batchsize)\n",
    "\n",
    "                    d = []\n",
    "                    for j in range(len(s)): # for each s[j]s\n",
    "                        ## if head 0 : append 0 at K, head 1 : append value of k to K\n",
    "                        ## iterate for all samples\n",
    "                        K=[]\n",
    "                        cS = s[j][0]; A = s[j][1]; R = s[j][2]; nS = s[j][3]; DONE = s[j][4]; heads = s[j][5];\n",
    "                        if not DONE:\n",
    "                            for n in range(len(heads)):\n",
    "                                if heads[n] == 0 : k = 0\n",
    "                                else: k = R + gamma*np.max(self.Qs[n][1].compute_Qvalues(nS)) #Qtarg_n\n",
    "                                K.append(k)\n",
    "                        elif DONE:\n",
    "                            for n in range(len(heads)):\n",
    "                                if heads[n] == 0 : k = 0\n",
    "                                else: k = R\n",
    "                                K.append(k)\n",
    "                        d.append(K)\n",
    "                    \n",
    "                    # update Qprins\n",
    "                    for n in range(self.ensemble_num):\n",
    "                        set_of_S = np.array([s[x][0] for x in range(len(s)) if s[x][5][n] == 1])\n",
    "                        set_of_A = np.array([s[x][1] for x in range(len(s)) if s[x][5][n] == 1])\n",
    "                        D = [d[x][n] for x in range(len(s)) if s[x][5][n] == 1]\n",
    "                        \n",
    "                        self.Qs[n][0].train(set_of_S, set_of_A, tf.convert_to_tensor(D, dtype=tf.float32))  # Qprin\n",
    "                #############################\n",
    "\n",
    "\n",
    "                #############################\n",
    "                ### Update theta of Qtarg ###\n",
    "                #############################\n",
    "                if totalstep % tau == 0:\n",
    "                    # print(\"\")\n",
    "                    # print(\"epsilon : \", eps)\n",
    "                    # print(\"target updated, totalstep : \", totalstep)\n",
    "                    for n in range(self.ensemble_num):\n",
    "                        self.Qs[n][1].update_weights(self.Qs[n][0])\n",
    "\n",
    "                #############################\n",
    "\n",
    "                pass\n",
    "            \n",
    "\n",
    "            self.r_record.append(rsum)\n",
    "            # if ite % 10 == 0:\n",
    "            #     print('iteration {} ave reward {}'.format(ite, np.mean(self.r_record[-10:])))\n",
    "            \n",
    "            #########################\n",
    "            ### Sample Efficiency ###\n",
    "            #########################\n",
    "            done = False\n",
    "            cum_reward = 0.0\n",
    "            s = self.env.reset()\n",
    "            while not done:\n",
    "                \n",
    "                voting_paper = np.zeros(self.env.action_space.n)\n",
    "\n",
    "                for n in range(self.ensemble_num):\n",
    "                    Q = self.Qs[n][0].compute_Qvalues(np.array(s))\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                    # voting_paper[action] += 1\n",
    "                    voting_paper[action] += (Q[0][action] - np.mean(Q[0]))/np.std(Q[0])\n",
    "                    \n",
    "                action = np.argmax(voting_paper)\n",
    "                ns, reward, done, _ = self.env.step(action)\n",
    "                cum_reward += reward\n",
    "                s = ns\n",
    "            self.AUC.append(cum_reward)\n",
    "\n",
    "            if ite == self.episode_length:\n",
    "                print(cum_reward)\n",
    "        \n",
    "        self.eps = eps\n",
    "\n",
    "        return self.AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTLdhIjQwSlR"
   },
   "source": [
    "chain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655253719489,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "t6pQCWOcwSlT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def seed_everything(seed): # seed 고정\n",
    "#     np.random.seed(seed)\n",
    "# #    random.seed(seed)\n",
    "# #    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "# head_num = 5\n",
    "# seeds = 20\n",
    "# for n in [100]:\n",
    "#     print(\"chain length:\", n)\n",
    "#     agents = []\n",
    "    \n",
    "#     for seed in range(seeds):\n",
    "#         seed_everything(seed)\n",
    "        \n",
    "#         env = ChainMDP(n)\n",
    "#         s = env.reset()\n",
    "#         agents.append(agent(env, head_num))\n",
    "#         AUC = agents[seed].train() #iteration: 100\n",
    "#         print(\"seed:\", seed)\n",
    "#         print(\"AUC: \", np.sum(AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o6mqeRLwSlT"
   },
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1655253719489,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "nQa-dxzfwSlT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WSy4mu2wSlU",
    "outputId": "73ef5ab1-7e14-4acf-9a47-87220424523e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:15:40<00:00,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 0.017000000000000008, 10.0, 10.0, 10.0, 10.0, 0.017000000000000008, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "AUC:  9550.306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from chain_mdp import ChainMDP\n",
    "# from agent_chainMDP import agent\n",
    "\n",
    "# chain length\n",
    "n = 10\n",
    "#head_num = 3\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "\"\"\" Your agent\"\"\"\n",
    "agents = agent(env)     # agent call\n",
    "AUC = agents.train()          # train policy of the agent\n",
    "print(AUC)\n",
    "print(\"AUC: \", np.sum(AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kZXCwmOcx3iB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "계산시간 : 4541.387456417084\n"
     ]
    }
   ],
   "source": [
    "print(\"계산시간 :\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZjYvofXfwSlU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state:  [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "state:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "action:  1\n",
      "total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "##### eval code #####\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "# always move right left: 0, right: 1\n",
    "\n",
    "env = ChainMDP(n)\n",
    "s = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agents.action(s)\n",
    "    print(\"state: \", s)\n",
    "    print(\"action: \", action)\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    s = ns\n",
    "    \n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "chainMDP_ensemble once.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
