{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dt8wywICwZAC"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uNDKkFNCwEhb"
   },
   "outputs": [],
   "source": [
    "class ChainMDP(gym.Env):\n",
    "    \"\"\"Chain MDP\n",
    "    The environment consists of a chain of N states and the agent always starts in state s2,\n",
    "    from where it can either move left or right.\n",
    "    In state s1, the agent receives a small reward of r = 0.001 by moving left.\n",
    "    A larger reward r = 1 is recived when moving right from state sN.\n",
    "    This environment is described in\n",
    "    Deep Exploration via Bootstrapped DQN(https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.state = 1  # start at s2\n",
    "        self.action_space = spaces.Discrete(2)  # {0, 1}\n",
    "        self.observation_space = spaces.Discrete(self.n)  # {0, 1, ... n-1}\n",
    "        self.max_nsteps = n + 8\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)  # assert 뒤의 조건이 True가 아니면 AssertError 발생\n",
    "        v = np.arange(self.n)  # [0, 1, ... n-1]\n",
    "        reward = lambda s, a: 1.0 if (s == (self.n - 1) and a == 1) else (0.001 if (s == 0 and a == 0) else 0)\n",
    "        is_done = lambda nsteps: nsteps >= self.max_nsteps  # True/False\n",
    "\n",
    "        r = reward(self.state, action)\n",
    "        if action:    # right\n",
    "            if self.state != self.n - 1:\n",
    "                self.state += 1\n",
    "        else:   # left\n",
    "            if self.state != 0:\n",
    "                self.state -= 1\n",
    "        self.nsteps += 1\n",
    "        return (v <= self.state).astype('float32'), r, is_done(self.nsteps), None\n",
    "\n",
    "    def reset(self):\n",
    "        v = np.arange(self.n)\n",
    "        self.state = 1\n",
    "        self.nsteps = 0\n",
    "        return (v <= self.state).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent _chainMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(keras.Model):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        hidden_dims: list containing output dimension of hidden layers \n",
    "        \"\"\"\n",
    "        super(Qfunction, self).__init__()\n",
    "\n",
    "        # Layer weight initializer\n",
    "        initializer = keras.initializers.RandomUniform(minval=-1., maxval=1.)\n",
    "\n",
    "        # Input Layer\n",
    "        self.input_layer = keras.layers.InputLayer(input_shape=(obssize,))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        self.hidden_layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            # TODO: define each hidden layers\n",
    "            layer = keras.layers.Dense(hidden_dim, activation='relu',\n",
    "                                      kernel_initializer=initializer)\n",
    "            self.hidden_layers.append(layer) \n",
    "        \n",
    "        # Output Layer : \n",
    "        # TODO: Define the output layer.\n",
    "        self.output_layer = keras.layers.Dense(actsize) \n",
    "\n",
    "    @tf.function\n",
    "    def call(self, states):\n",
    "        ########################################################################\n",
    "        # TODO: You SHOULD implement the model's forward pass\n",
    "        x = self.input_layer(states)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        ########################################################################\n",
    "\n",
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "        self.qfunction = Qfunction(obssize, actsize, hidden_dims)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        \"\"\"\n",
    "        states represent s_t\n",
    "        actions represent a_t\n",
    "        \"\"\"\n",
    "        ########################################################################\n",
    "        # TODO: Define the logic for calculate  Q_\\theta(s,a)\n",
    "        q = []\n",
    "        for j in range(len(actions)):\n",
    "           q.append(self.qfunction(states)[j][actions[j]])\n",
    "        return tf.convert_to_tensor(q, dtype=tf.float32)\n",
    "        ########################################################################\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        \"\"\"\n",
    "        Qpreds represent Q_\\theta(s,a)\n",
    "        targets represent the terms E[r+gamma Q] in Bellman equations\n",
    "        This function is OBJECTIVE function\n",
    "        \"\"\"\n",
    "        l = tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "        return l\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        inputs = np.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        \"\"\"\n",
    "        We need a subroutine to update target network \n",
    "        i.e. to copy from principal network to target network. \n",
    "        This function is for copying  theta -> theta target \n",
    "        \"\"\"\n",
    "        \n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        # soft assign\n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(0.8*v1+0.2*v2)\n",
    "\n",
    "# Implement replay buffer\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pXIbA5RBwTLA"
   },
   "outputs": [],
   "source": [
    "# class agent():\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         return\n",
    "    \n",
    "#     def action(self):\n",
    "        \n",
    "#         return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DQN implementation ###\n",
    "class agent():\n",
    "    \n",
    "    def __init__(self, e):\n",
    "\n",
    "        self.env = e\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        ### For Q value training ###        \n",
    "        self.episode_length = 1000\n",
    "        self.hidden_dim = [8, 4]\n",
    "        self.lr = 5e-4\n",
    "        self.Qprin = DQN(10, e.action_space.n, [8, 4], optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        self.Qtarg = DQN(10, e.action_space.n, [8, 4], optimizer = keras.optimizers.Adam(learning_rate=self.lr))\n",
    "        ############################\n",
    "\n",
    "        return\n",
    "    \n",
    "    def action(self):\n",
    "        \n",
    "        Q = self.Qprin.compute_Qvalues(np.array(self.state))\n",
    "        action = np.argmax(Q)   # always max action choose\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        ### For Q value training ###\n",
    "        totalstep = 0\n",
    "        initialize = 500\n",
    "        eps = 1; eps_minus = 1e-4\n",
    "        tau = 100\n",
    "        gamma = 0.99\n",
    "        batchsize = 64\n",
    "        buff_max_size = 10000\n",
    "        buffer = ReplayBuffer(buff_max_size)\n",
    "        ############################\n",
    "\n",
    "        r_record = []\n",
    "\n",
    "        for ite in range(self.episode_length):\n",
    "            self.state = self.env.reset()\n",
    "            done = False\n",
    "            rsum = 0\n",
    "\n",
    "            while not done:\n",
    "                totalstep += 1\n",
    "\n",
    "                if eps > 0.05 and totalstep > initialize: eps -= eps_minus\n",
    "                elif eps < 0.05 and totalstep > initialize: eps = 0.05\n",
    "\n",
    "                ##################\n",
    "                ### Get Action ###\n",
    "                ##################\n",
    "                if np.random.rand() < eps or totalstep <= initialize:\n",
    "                    action = np.random.choice([0, 1])\n",
    "                else:\n",
    "                    Q = self.Qprin.compute_Qvalues(np.array(self.state))\n",
    "                    action = np.argmax(Q)   # always max action choose\n",
    "                ##################\n",
    "\n",
    "                ##################\n",
    "                ###  ONE STEP  ###\n",
    "                ##################\n",
    "                curr_state = self.state\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                rsum += reward\n",
    "                ##################\n",
    "                \n",
    "                #####################\n",
    "                ### Update Buffer ###\n",
    "                #####################\n",
    "                buffer.append((curr_state, action, reward, next_state, done))\n",
    "                #####################\n",
    "\n",
    "                #############################\n",
    "                ### N Samples from Buffer ###\n",
    "                ###         and           ###\n",
    "                ### Update theta of Qprin ###\n",
    "                #############################\n",
    "                if totalstep > initialize:\n",
    "\n",
    "                    # sample\n",
    "                    s = buffer.sample(batchsize)\n",
    "\n",
    "                    d = []\n",
    "                    for j in range(len(s)):\n",
    "                        cS = s[j][0]; A = s[j][1]; R = s[j][2]; nS = s[j][3]; DONE = s[j][4];\n",
    "                        if not DONE:\n",
    "                            k = R + gamma*np.max(self.Qtarg.compute_Qvalues(nS))\n",
    "                        elif DONE:\n",
    "                            k = R\n",
    "                        d.append(k)\n",
    "                    \n",
    "                    set_of_S = np.array([s[x][0] for x in range(len(s))])\n",
    "                    set_of_A = np.array([s[x][1] for x in range(len(s))])\n",
    "                    loss = self.Qprin.train(set_of_S, set_of_A, tf.convert_to_tensor(d, dtype=tf.float32))\n",
    "                #############################\n",
    "\n",
    "\n",
    "                #############################\n",
    "                ### Update theta of Qtarg ###\n",
    "                #############################\n",
    "                if totalstep % tau == 0:\n",
    "                    print(\"\")\n",
    "                    print(\"epsilon : \", eps)\n",
    "                    print(\"target updated, totalstep : \", totalstep)\n",
    "                    self.Qtarg.update_weights(self.Qprin)\n",
    "                #############################\n",
    "\n",
    "                pass\n",
    "            \n",
    "\n",
    "            r_record.append(rsum)\n",
    "            if ite % 10 == 0:\n",
    "                print('iteration {} ave reward {}'.format(ite, np.mean(r_record[-10:])))\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653541582188,
     "user": {
      "displayName": "­서성민 / 학생 / 기계공학부",
      "userId": "08487562865148337619"
     },
     "user_tz": -540
    },
    "id": "Q2AxqGtmwVO3",
    "outputId": "b4a725c2-fcfa-4b78-a591-2bdf8a152aa9"
   },
   "outputs": [],
   "source": [
    "# from chain_mdp import ChainMDP\n",
    "# from agent_chainMDP import agent\n",
    "\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "env = ChainMDP(10)\n",
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ave reward 0.003\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  100\n",
      "iteration 10 ave reward 0.3013\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  200\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  300\n",
      "iteration 20 ave reward 0.0020000000000000005\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  400\n",
      "\n",
      "epsilon :  1\n",
      "target updated, totalstep :  500\n",
      "iteration 30 ave reward 0.2011\n",
      "\n",
      "epsilon :  0.9900000000000011\n",
      "target updated, totalstep :  600\n",
      "\n",
      "epsilon :  0.9800000000000022\n",
      "target updated, totalstep :  700\n",
      "iteration 40 ave reward 0.0015\n",
      "\n",
      "epsilon :  0.9700000000000033\n",
      "target updated, totalstep :  800\n",
      "\n",
      "epsilon :  0.9600000000000044\n",
      "target updated, totalstep :  900\n",
      "iteration 50 ave reward 0.0033\n",
      "\n",
      "epsilon :  0.9500000000000055\n",
      "target updated, totalstep :  1000\n",
      "iteration 60 ave reward 0.2028\n",
      "\n",
      "epsilon :  0.9400000000000066\n",
      "target updated, totalstep :  1100\n",
      "\n",
      "epsilon :  0.9300000000000077\n",
      "target updated, totalstep :  1200\n",
      "iteration 70 ave reward 0.20229999999999998\n",
      "\n",
      "epsilon :  0.9200000000000088\n",
      "target updated, totalstep :  1300\n",
      "\n",
      "epsilon :  0.9100000000000099\n",
      "target updated, totalstep :  1400\n",
      "iteration 80 ave reward 0.0024000000000000002\n",
      "\n",
      "epsilon :  0.900000000000011\n",
      "target updated, totalstep :  1500\n",
      "\n",
      "epsilon :  0.8900000000000121\n",
      "target updated, totalstep :  1600\n",
      "iteration 90 ave reward 0.0039000000000000007\n",
      "\n",
      "epsilon :  0.8800000000000132\n",
      "target updated, totalstep :  1700\n",
      "\n",
      "epsilon :  0.8700000000000143\n",
      "target updated, totalstep :  1800\n",
      "iteration 100 ave reward 0.0036000000000000003\n",
      "\n",
      "epsilon :  0.8600000000000154\n",
      "target updated, totalstep :  1900\n",
      "iteration 110 ave reward 0.0024000000000000002\n",
      "\n",
      "epsilon :  0.8500000000000165\n",
      "target updated, totalstep :  2000\n",
      "\n",
      "epsilon :  0.8400000000000176\n",
      "target updated, totalstep :  2100\n",
      "iteration 120 ave reward 0.0017000000000000001\n",
      "\n",
      "epsilon :  0.8300000000000187\n",
      "target updated, totalstep :  2200\n",
      "\n",
      "epsilon :  0.8200000000000198\n",
      "target updated, totalstep :  2300\n",
      "iteration 130 ave reward 0.5006\n",
      "\n",
      "epsilon :  0.8100000000000209\n",
      "target updated, totalstep :  2400\n",
      "\n",
      "epsilon :  0.800000000000022\n",
      "target updated, totalstep :  2500\n",
      "iteration 140 ave reward 0.0017000000000000001\n",
      "\n",
      "epsilon :  0.7900000000000231\n",
      "target updated, totalstep :  2600\n",
      "\n",
      "epsilon :  0.7800000000000242\n",
      "target updated, totalstep :  2700\n",
      "iteration 150 ave reward 0.30119999999999997\n",
      "\n",
      "epsilon :  0.7700000000000253\n",
      "target updated, totalstep :  2800\n",
      "iteration 160 ave reward 0.0016\n",
      "\n",
      "epsilon :  0.7600000000000264\n",
      "target updated, totalstep :  2900\n",
      "\n",
      "epsilon :  0.7500000000000275\n",
      "target updated, totalstep :  3000\n",
      "iteration 170 ave reward 0.6006\n",
      "\n",
      "epsilon :  0.7400000000000286\n",
      "target updated, totalstep :  3100\n",
      "\n",
      "epsilon :  0.7300000000000297\n",
      "target updated, totalstep :  3200\n",
      "iteration 180 ave reward 0.7011999999999999\n",
      "\n",
      "epsilon :  0.7200000000000308\n",
      "target updated, totalstep :  3300\n",
      "\n",
      "epsilon :  0.7100000000000319\n",
      "target updated, totalstep :  3400\n",
      "iteration 190 ave reward 0.20029999999999998\n",
      "\n",
      "epsilon :  0.700000000000033\n",
      "target updated, totalstep :  3500\n",
      "\n",
      "epsilon :  0.6900000000000341\n",
      "target updated, totalstep :  3600\n",
      "iteration 200 ave reward 0.5005\n",
      "\n",
      "epsilon :  0.6800000000000352\n",
      "target updated, totalstep :  3700\n",
      "iteration 210 ave reward 0.8009000000000001\n",
      "\n",
      "epsilon :  0.6700000000000363\n",
      "target updated, totalstep :  3800\n",
      "\n",
      "epsilon :  0.6600000000000374\n",
      "target updated, totalstep :  3900\n",
      "iteration 220 ave reward 1.6001999999999998\n",
      "\n",
      "epsilon :  0.6500000000000385\n",
      "target updated, totalstep :  4000\n",
      "\n",
      "epsilon :  0.6400000000000396\n",
      "target updated, totalstep :  4100\n",
      "iteration 230 ave reward 1.6000999999999999\n",
      "\n",
      "epsilon :  0.6300000000000407\n",
      "target updated, totalstep :  4200\n",
      "\n",
      "epsilon :  0.6200000000000419\n",
      "target updated, totalstep :  4300\n",
      "iteration 240 ave reward 1.7003999999999997\n",
      "\n",
      "epsilon :  0.610000000000043\n",
      "target updated, totalstep :  4400\n",
      "\n",
      "epsilon :  0.600000000000044\n",
      "target updated, totalstep :  4500\n",
      "iteration 250 ave reward 1.301\n",
      "\n",
      "epsilon :  0.5900000000000452\n",
      "target updated, totalstep :  4600\n",
      "iteration 260 ave reward 0.20039999999999997\n",
      "\n",
      "epsilon :  0.5800000000000463\n",
      "target updated, totalstep :  4700\n",
      "\n",
      "epsilon :  0.5700000000000474\n",
      "target updated, totalstep :  4800\n",
      "iteration 270 ave reward 1.0003000000000002\n",
      "\n",
      "epsilon :  0.5600000000000485\n",
      "target updated, totalstep :  4900\n",
      "\n",
      "epsilon :  0.5500000000000496\n",
      "target updated, totalstep :  5000\n",
      "iteration 280 ave reward 2.5004999999999997\n",
      "\n",
      "epsilon :  0.5400000000000507\n",
      "target updated, totalstep :  5100\n",
      "\n",
      "epsilon :  0.5300000000000518\n",
      "target updated, totalstep :  5200\n",
      "iteration 290 ave reward 1.9000999999999997\n",
      "\n",
      "epsilon :  0.5200000000000529\n",
      "target updated, totalstep :  5300\n",
      "\n",
      "epsilon :  0.510000000000054\n",
      "target updated, totalstep :  5400\n",
      "iteration 300 ave reward 2.9\n",
      "\n",
      "epsilon :  0.5000000000000551\n",
      "target updated, totalstep :  5500\n",
      "iteration 310 ave reward 1.9\n",
      "\n",
      "epsilon :  0.49000000000005617\n",
      "target updated, totalstep :  5600\n",
      "\n",
      "epsilon :  0.48000000000005727\n",
      "target updated, totalstep :  5700\n",
      "iteration 320 ave reward 5.1\n",
      "\n",
      "epsilon :  0.47000000000005837\n",
      "target updated, totalstep :  5800\n",
      "\n",
      "epsilon :  0.4600000000000595\n",
      "target updated, totalstep :  5900\n",
      "iteration 330 ave reward 1.9000999999999997\n",
      "\n",
      "epsilon :  0.4500000000000606\n",
      "target updated, totalstep :  6000\n",
      "\n",
      "epsilon :  0.4400000000000617\n",
      "target updated, totalstep :  6100\n",
      "iteration 340 ave reward 2.3003\n",
      "\n",
      "epsilon :  0.4300000000000628\n",
      "target updated, totalstep :  6200\n",
      "\n",
      "epsilon :  0.4200000000000639\n",
      "target updated, totalstep :  6300\n",
      "iteration 350 ave reward 4.0001\n",
      "\n",
      "epsilon :  0.410000000000065\n",
      "target updated, totalstep :  6400\n",
      "iteration 360 ave reward 2.9\n",
      "\n",
      "epsilon :  0.4000000000000661\n",
      "target updated, totalstep :  6500\n",
      "\n",
      "epsilon :  0.3900000000000672\n",
      "target updated, totalstep :  6600\n",
      "iteration 370 ave reward 3.2\n",
      "\n",
      "epsilon :  0.3800000000000683\n",
      "target updated, totalstep :  6700\n",
      "\n",
      "epsilon :  0.3700000000000694\n",
      "target updated, totalstep :  6800\n",
      "iteration 380 ave reward 4.2\n",
      "\n",
      "epsilon :  0.3600000000000705\n",
      "target updated, totalstep :  6900\n",
      "\n",
      "epsilon :  0.3500000000000716\n",
      "target updated, totalstep :  7000\n",
      "iteration 390 ave reward 5.0001\n",
      "\n",
      "epsilon :  0.3400000000000727\n",
      "target updated, totalstep :  7100\n",
      "\n",
      "epsilon :  0.3300000000000738\n",
      "target updated, totalstep :  7200\n",
      "iteration 400 ave reward 5.8\n",
      "\n",
      "epsilon :  0.3200000000000749\n",
      "target updated, totalstep :  7300\n",
      "iteration 410 ave reward 3.9\n",
      "\n",
      "epsilon :  0.310000000000076\n",
      "target updated, totalstep :  7400\n",
      "\n",
      "epsilon :  0.3000000000000771\n",
      "target updated, totalstep :  7500\n",
      "iteration 420 ave reward 5.1\n",
      "\n",
      "epsilon :  0.2900000000000782\n",
      "target updated, totalstep :  7600\n",
      "\n",
      "epsilon :  0.2800000000000793\n",
      "target updated, totalstep :  7700\n",
      "iteration 430 ave reward 6.6\n",
      "\n",
      "epsilon :  0.2700000000000804\n",
      "target updated, totalstep :  7800\n",
      "\n",
      "epsilon :  0.2600000000000815\n",
      "target updated, totalstep :  7900\n",
      "iteration 440 ave reward 5.1\n",
      "\n",
      "epsilon :  0.2500000000000826\n",
      "target updated, totalstep :  8000\n",
      "\n",
      "epsilon :  0.2400000000000837\n",
      "target updated, totalstep :  8100\n",
      "iteration 450 ave reward 5.9\n",
      "\n",
      "epsilon :  0.2300000000000848\n",
      "target updated, totalstep :  8200\n",
      "iteration 460 ave reward 5.4\n",
      "\n",
      "epsilon :  0.2200000000000859\n",
      "target updated, totalstep :  8300\n",
      "\n",
      "epsilon :  0.210000000000087\n",
      "target updated, totalstep :  8400\n",
      "iteration 470 ave reward 7.7\n",
      "\n",
      "epsilon :  0.2000000000000881\n",
      "target updated, totalstep :  8500\n",
      "\n",
      "epsilon :  0.1900000000000892\n",
      "target updated, totalstep :  8600\n",
      "iteration 480 ave reward 6.3\n",
      "\n",
      "epsilon :  0.1800000000000903\n",
      "target updated, totalstep :  8700\n",
      "\n",
      "epsilon :  0.1700000000000914\n",
      "target updated, totalstep :  8800\n",
      "iteration 490 ave reward 7.9\n",
      "\n",
      "epsilon :  0.1600000000000925\n",
      "target updated, totalstep :  8900\n",
      "\n",
      "epsilon :  0.15000000000009361\n",
      "target updated, totalstep :  9000\n",
      "iteration 500 ave reward 8.7\n",
      "\n",
      "epsilon :  0.14000000000009472\n",
      "target updated, totalstep :  9100\n",
      "iteration 510 ave reward 7.2\n",
      "\n",
      "epsilon :  0.13000000000009582\n",
      "target updated, totalstep :  9200\n",
      "\n",
      "epsilon :  0.12000000000009622\n",
      "target updated, totalstep :  9300\n",
      "iteration 520 ave reward 6.9001\n",
      "\n",
      "epsilon :  0.11000000000009594\n",
      "target updated, totalstep :  9400\n",
      "\n",
      "epsilon :  0.10000000000009565\n",
      "target updated, totalstep :  9500\n",
      "iteration 530 ave reward 9.2\n",
      "\n",
      "epsilon :  0.09000000000009536\n",
      "target updated, totalstep :  9600\n",
      "\n",
      "epsilon :  0.08000000000009508\n",
      "target updated, totalstep :  9700\n",
      "iteration 540 ave reward 8.0\n",
      "\n",
      "epsilon :  0.07000000000009479\n",
      "target updated, totalstep :  9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epsilon :  0.060000000000094506\n",
      "target updated, totalstep :  9900\n",
      "iteration 550 ave reward 8.4\n",
      "\n",
      "epsilon :  0.05000000000009422\n",
      "target updated, totalstep :  10000\n",
      "iteration 560 ave reward 9.8\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10100\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10200\n",
      "iteration 570 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10300\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10400\n",
      "iteration 580 ave reward 9.3\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10500\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10600\n",
      "iteration 590 ave reward 9.8\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10700\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10800\n",
      "iteration 600 ave reward 9.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  10900\n",
      "iteration 610 ave reward 9.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11000\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11100\n",
      "iteration 620 ave reward 8.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11200\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11300\n",
      "iteration 630 ave reward 9.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11400\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11500\n",
      "iteration 640 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11600\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11700\n",
      "iteration 650 ave reward 8.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11800\n",
      "iteration 660 ave reward 8.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  11900\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12000\n",
      "iteration 670 ave reward 8.8\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12100\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12200\n",
      "iteration 680 ave reward 9.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12300\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12400\n",
      "iteration 690 ave reward 9.3\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12500\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12600\n",
      "iteration 700 ave reward 8.5\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12700\n",
      "iteration 710 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12800\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  12900\n",
      "iteration 720 ave reward 8.7\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13000\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13100\n",
      "iteration 730 ave reward 9.8\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13200\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13300\n",
      "iteration 740 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13400\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13500\n",
      "iteration 750 ave reward 9.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13600\n",
      "iteration 760 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13700\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13800\n",
      "iteration 770 ave reward 8.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  13900\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14000\n",
      "iteration 780 ave reward 9.7\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14100\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14200\n",
      "iteration 790 ave reward 9.1\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14300\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14400\n",
      "iteration 800 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14500\n",
      "iteration 810 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14600\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14700\n",
      "iteration 820 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14800\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  14900\n",
      "iteration 830 ave reward 8.6\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15000\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15100\n",
      "iteration 840 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15200\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15300\n",
      "iteration 850 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15400\n",
      "iteration 860 ave reward 8.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15500\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15600\n",
      "iteration 870 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15700\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15800\n",
      "iteration 880 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  15900\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16000\n",
      "iteration 890 ave reward 9.5\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16100\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16200\n",
      "iteration 900 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16300\n",
      "iteration 910 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16400\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16500\n",
      "iteration 920 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16600\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16700\n",
      "iteration 930 ave reward 9.5\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16800\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  16900\n",
      "iteration 940 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17000\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17100\n",
      "iteration 950 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17200\n",
      "iteration 960 ave reward 9.0\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17300\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17400\n",
      "iteration 970 ave reward 9.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17500\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17600\n",
      "iteration 980 ave reward 8.2\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17700\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17800\n",
      "iteration 990 ave reward 9.4\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  17900\n",
      "\n",
      "epsilon :  0.05\n",
      "target updated, totalstep :  18000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent(env)     # agent call\n",
    "agent.train()          # train policy of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "##### eval code #####\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "# always move right left: 0, right: 1\n",
    "action = 1\n",
    "\n",
    "while not done:    \n",
    "    action = agent.action()\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "    s = ns\n",
    "    \n",
    "print(f\"total reward: {cum_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMpfCYx8n6uiaJryHpETH/m",
   "name": "chainMDP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
